{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import glob\n",
    "from typing import List\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "#API_KEY = \"YOUR_TONG_API_KEY\"\n",
    "#os.environ[\"AZURE_OPENAI_API_KEY\"] = API_KEY\n",
    "\n",
    "REGION = \"canadaeast\"\n",
    "API_BASE = \"https://api.tonggpt.mybigai.ac.cn/proxy\"\n",
    "ENDPOINT = f\"{API_BASE}/{REGION}\"\n",
    "\n",
    "openai.azure_endpoint = ENDPOINT\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = ENDPOINT\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "template_cn = \"\"\"\n",
    "你是一个问答机器人，使用下面提供的信息进行归纳和总结来回答最后的问题。如果你不知道答案，清回答“我不知道”，不要试图编造答案或使用未被提供的信息。最多输出3句话，尽可能让回答简单明了\"\n",
    "{context}\n",
    "问题：{question}\n",
    "回答：\"\"\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo-0125\",  # or your deployment\n",
    "    temperature=0)\n",
    "\n",
    "class DefaultChromaEmbedding(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.default_ef(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.default_ef([text])[0]\n",
    "\n",
    "chroma_embedding = DefaultChromaEmbedding()\n",
    "\n",
    "class KnowledgeBaseBuilder:\n",
    "    def __init__(self, text_splitter=None, embedding_func=None, persist_dir=\"docs/data/chroma_dev/\"):\n",
    "        if not text_splitter:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=100,\n",
    "                separators=[\n",
    "                    \"\\n\\n\",\n",
    "                    \"\\n\",\n",
    "                    \" \",\n",
    "                    \".\",\n",
    "                    \",\",\n",
    "                    \"\\u200b\",  # Zero-width space\n",
    "                    \"\\uff0c\",  # Fullwidth comma\n",
    "                    \"\\u3001\",  # Ideographic comma\n",
    "                    \"\\uff0e\",  # Fullwidth full stop\n",
    "                    \"\\u3002\",  # Ideographic full stop\n",
    "                    \"\",\n",
    "                ],\n",
    "            )\n",
    "        if not embedding_func:\n",
    "            embedding_func = DefaultChromaEmbedding()\n",
    "        self.text_splitter = text_splitter\n",
    "        self.embedding_func = embedding_func\n",
    "        self.persist_dir = persist_dir\n",
    "        self.vectordb = None\n",
    "\n",
    "    def docs_preprocess(self, path_name):\n",
    "        loaders = [\n",
    "            PyPDFLoader(doc_path) for doc_path in glob.glob(path_name)\n",
    "        ]\n",
    "        docs = []\n",
    "        for loader in loaders:\n",
    "            docs.extend(loader.load())\n",
    "        return self.text_splitter.split_documents(docs)\n",
    "\n",
    "    def knowledge_base_build_chain(self, path_name):\n",
    "        splits = self.docs_preprocess(path_name)\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=self.embedding_func,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "        return vectordb\n",
    "\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, vectordb):\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            output_key=\"result\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=vectordb.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "            output_key=\"result\",\n",
    "            #memory=memory,\n",
    "            chain_type_kwargs={\"prompt\": PromptTemplate.from_template(template_cn)}\n",
    "        )\n",
    "\n",
    "    def query(self, question):\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        return result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"docs/data/chroma_dev\"\n",
    "\n",
    "# Load existing database\n",
    "# vectordb = Chroma(\n",
    "#     persist_directory=persist_directory,\n",
    "#     embedding_function=chroma_embedding\n",
    "# )\n",
    "\n",
    "# OR create new vector database\n",
    "# remove old database files if any\n",
    "import shutil\n",
    "import os\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "knowledge_base_builder = KnowledgeBaseBuilder(persist_dir=persist_directory)\n",
    "vectordb = knowledge_base_builder.knowledge_base_build_chain(\"docs/bigai/*.pdf\")\n",
    "#vectordb.persist()\n",
    "qa_chain_dev = QAChain(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"试用期导师的职责\"\n",
    "result = qa_chain_dev.query(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"招聘新员工的渠道\"\n",
    "result = qa_chain_dev.query(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
